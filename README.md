#### Group members

Mostafa Allahmoradi - 9087818

Jarius Bedward - 8841640


#### Database Description
- Dataset: We use the Brown Corpus & Gutenberg which is a NLTK corpus that is a collection of English literary texts from multiple books. 
- Size: Brown: ~1 Million words ~57,000 Sentences. Gutenberg: 80,000 free ebooks with ~80,000 words per book which is about 6.4 billion words
- Structure: Brown: Texts are split into categories(genres). Gutenberg: Split into different classic english books
- We use the brown corpus to train word embeddings, using sentences from all categories and use Gutenberg for test
- Link: https://www.gutenberg.org/

- We also used the Glove Corpus available at http://nlp.stanford.edu/data/glove.6B.zip  
