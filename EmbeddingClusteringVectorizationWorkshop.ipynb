{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbaa7c9b7c213709",
   "metadata": {},
   "source": [
    "#### Group members\n",
    "\n",
    "Mostafa Allahmoradi - 9087818\n",
    "Jarius Bedward - 8841640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf81e7a2296518",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0edd9ec9965389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:42:56.122027Z",
     "start_time": "2025-11-28T19:42:56.116828Z"
    }
   },
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus.reader import documents\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow\n",
    "from tensorflow.python.types.doc_typealias import document"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "15f202fb127d2935",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "55892887ee20316f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:43:02.878944Z",
     "start_time": "2025-11-28T19:42:56.143749Z"
    }
   },
   "source": [
    "# Warning: This download will copy files to your home directory.\n",
    "# For example, on Linux, it will copy files to ~/.nltk_data.\n",
    "# In Windows, it will copy files to C:\\Users\\YourAccount\\AppData\\Roaming\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# A better way to handle the download is to:\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "print(\"Downloading tokenizer resources...\")\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path, force=True)\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# makes sure path is used by nltk\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "print(\"Active nltk paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\jjbed/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jjbed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\jjbed\\Downloads\\ML\n",
      "[nltk_data]     prog lab9\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jjbed\\Downloads\\ML prog lab9\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active nltk paths: ['C:\\\\Users\\\\jjbed/nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\Downloads\\\\ML prog lab9\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "1ffb7c99f3e8dc2b",
   "metadata": {},
   "source": "## Document Collection\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:43:03.065235Z",
     "start_time": "2025-11-28T19:43:02.909164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "documents = [\" \".join(sent) for sent in brown.sents()[:500]] #using first 500 for demo, borwn.sents gives brown corpus as list of sentencess\n",
    "\n",
    "\n",
    "print (\"Number of documents collected:\", len(documents)) #length of the list\n",
    "print(\"Sample document:\\n\", documents[0])"
   ],
   "id": "a8dff416d88cac0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents collected: 500\n",
      "Sample document:\n",
      " The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Use of brown corpus to get a list of sentences using brown.sents\n",
    "- len(documents) gives us the length of the list which is how many documents we have"
   ],
   "id": "a7db8ce52dfd6c36"
  },
  {
   "cell_type": "markdown",
   "id": "59002f9396acb348",
   "metadata": {},
   "source": [
    "##  Tokenizer, Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "31caa514b18fe811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:43:03.175028Z",
     "start_time": "2025-11-28T19:43:03.089290Z"
    }
   },
   "source": [
    "#Normalization\n",
    "\n",
    "def normalize(text):\n",
    "    # in lowercase text\n",
    "    text = text.lower()\n",
    "    #removes punctionation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #removes numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    #Removes urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    #removes extra white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "normalize_docs = [normalize(doc) for doc in documents] #take each document from document list  and apply normalization\n",
    "\n",
    "print(\"Normalized Sample:\\n\", normalize_docs[0])\n",
    "\n",
    "#Tokenization Pipeline\n",
    "\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "tokenize_docs = [tokenize(doc) for doc in normalize_docs] # use normalize docs to tokenize the normalized words\n",
    " #take each document from document list  and apply tokenization\n",
    "\n",
    "#FInal output print\n",
    "print(\"Tokenized sample:\\n\", tokenize_docs[0])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Sample:\n",
      " the fulton county grand jury said friday an investigation of atlantas recent primary election produced no evidence that any irregularities took place\n",
      "Tokenized sample:\n",
      " ['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Normalization\n",
    "- We create a normalization class and normalize the text by transforming to lowercase, removes punctuation, removing numbers, removes urls, and removes extra white spaces using regex\n",
    "- Then we take the list of documents and apply normalization to the whole list\n",
    "###### Tokenization\n",
    "- We then apply tokenization by removing stop words in a loop\n",
    "- Then we use the normalized list to tokenize the already normalized words"
   ],
   "id": "b66b770c90ab1f98"
  },
  {
   "cell_type": "markdown",
   "id": "ab1cda67003e6706",
   "metadata": {},
   "source": [
    "## Implement a Word2Vec predictive model using the knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce278b73497742e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:43:03.468536Z",
     "start_time": "2025-11-28T19:43:03.186752Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "model_word2vec = Word2Vec(\n",
    "    sentences=tokenize_docs, # the tokenized corpus must be a list of lists\n",
    "    vector_size=100,    #size of embedding\n",
    "   window=5,        #context window\n",
    "   min_count=1,     #keep all words (for demo purpose\n",
    "    workers=4,         #choose how much cpu coreses use\n",
    "  sg = 1            # number of skip-grams = 1 since this is small data 1\n",
    ")\n",
    "\n",
    "print(\"Words2Vec model trained\")\n",
    "\n",
    "#xample check\n",
    "#checks if money exists in corpus\n",
    "word = \"money\"\n",
    "vector_word2Vec = model_word2vec.wv[word] #word vector store inside w2v model\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_word2Vec}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words2Vec model trained\n",
      "Word vector for 'money' using Word2Vec: [ 0.00213936 -0.00513772  0.00957934 -0.00377395 -0.00815433 -0.00564388\n",
      "  0.00576452  0.00625592  0.00330616  0.00837108 -0.00678436  0.00397323\n",
      "  0.00097306  0.00790533  0.00656204  0.00115702  0.00996519 -0.01281296\n",
      "  0.00022501  0.00131961  0.0115654   0.00826382  0.00155255 -0.00216188\n",
      "  0.00765863  0.00422351  0.00334267 -0.00833297 -0.00199157 -0.00869753\n",
      "  0.00224814 -0.00806683 -0.00173035  0.00755662 -0.00856033  0.01140152\n",
      " -0.00545522  0.00373608  0.00374476 -0.011393   -0.00721612 -0.00719035\n",
      " -0.00944927 -0.00501105  0.00360177 -0.00920607 -0.01294341  0.00538896\n",
      "  0.00534256 -0.00224518  0.00428087 -0.00349704  0.00885019  0.00220814\n",
      "  0.00173936 -0.00806178 -0.00615809 -0.00350766  0.00408438 -0.00687911\n",
      "  0.00386494  0.00066182 -0.00565066  0.0051219  -0.00864518  0.01070338\n",
      " -0.00858912  0.0035071  -0.00486206 -0.00380146  0.00038248 -0.00548745\n",
      "  0.0086353  -0.00861872 -0.00439417 -0.00572507  0.00331686 -0.00089664\n",
      " -0.00663668  0.00747407 -0.00962488  0.00266302 -0.00987371  0.00194879\n",
      "  0.00256795 -0.00603824  0.00137333  0.01120189  0.00577709 -0.00295305\n",
      " -0.00248077 -0.00543044  0.01006346 -0.00259929  0.00758219  0.01279009\n",
      "  0.00764215 -0.01174881  0.00996185  0.0044549 ]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The words2vec model is trained on the tokenized corpus to learn dense vector relationships for each word capturing contextual and semantic relationships.\n",
    "- The parameters like vector_size=100 and window=5 control the embedding dimension and context window while sg=1 is the skip gram and uses the skipgram approach to predict surrounding words from a target word\n",
    "- Once trained, each word in the vocab can be represented as a numerical vector which can be used for other tasks like sentiment analysis, text classification, and analogy reasoning"
   ],
   "id": "95dcf73f74b0a241"
  },
  {
   "cell_type": "markdown",
   "id": "f7539d21",
   "metadata": {},
   "source": [
    "#### Implement a GloVe count-based model using the knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f62ab1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:43:03.552547Z",
     "start_time": "2025-11-28T19:43:03.487118Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from glove import Glove, Corpus\n",
    "import numpy as np\n",
    "\n",
    "sentences = [['this', 'is', 'an', 'example'], ['glove', 'is', 'awesome']]\n",
    "\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=5)\n",
    "\n",
    "glove_model = Glove(no_components=100, learning_rate=0.05)\n",
    "glove_model.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove_model.add_dictionary(corpus.dictionary)\n",
    "\n",
    "print(glove_model.word_vectors[glove_model.dictionary['glove']])\n",
    "print(glove_model.most_similar('glove'))\n",
    "\n",
    "#Download Glove Pretrained Embeddings From: http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "\n",
    "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index.index(word)\n",
    "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "                \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "# matrix for vocab: tokenized_words\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab('../glove.6B.50d/glove.6B.50d.txt', tokenized_words, embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \", embedding_matrix_vocab[1])"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mglove\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Glove, Corpus\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      4\u001B[39m sentences = [[\u001B[33m'\u001B[39m\u001B[33mthis\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33man\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mexample\u001B[39m\u001B[33m'\u001B[39m], [\u001B[33m'\u001B[39m\u001B[33mglove\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mawesome\u001B[39m\u001B[33m'\u001B[39m]]\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'glove'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "89ab523e1f9dccdb",
   "metadata": {},
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Teams of 2 (individual evaluation in class).\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into markdown comments.\n",
    "\n",
    "\n",
    "## ðŸ§© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** â€“ Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* â€“ NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** â€“ Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
