{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbaa7c9b7c213709",
   "metadata": {},
   "source": [
    "#### Group members\n",
    "\n",
    "Mostafa Allahmoradi - 9087818\n",
    "Jarius Bedward - 8841640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf81e7a2296518",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0edd9ec9965389",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc_typealias\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m document\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus.reader import documents\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.python.types.doc_typealias import document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f202fb127d2935",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55892887ee20316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "print(\"Downloading tokenizer resources...\")\n",
    "\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path, force=True)\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# makes sure path is used by nltk\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "print(\"Active nltk paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb7c99f3e8dc2b",
   "metadata": {},
   "source": [
    "## Document Collection\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59002f9396acb348",
   "metadata": {},
   "source": [
    "##  Tokenizer, Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31caa514b18fe811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "def normalize(text):\n",
    "    # in lowercase text\n",
    "    text = text.lower()\n",
    "    #removes punctionation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #removes numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    #Removes urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    #removes extra white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "normalize_docs = [normalize(doc) for doc in documents]\n",
    "\n",
    "#Tokenization\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "tokenize_docs = [tokenize(doc) for doc in normalize_docs]\n",
    "\n",
    "#FInal output print\n",
    "print(\"Original:\")\n",
    "print(documents, \"\\n\")\n",
    "\n",
    "print(\"Normalized:\")\n",
    "print(normalize_docs, \"\\n\")\n",
    "\n",
    "print(\"Tokenized:\")\n",
    "print(tokenize_docs, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cda67003e6706",
   "metadata": {},
   "source": [
    "## Implement a Word2Vec predictive model using the knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce278b73497742e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenize_docs, # the tokenized corpus must be a list of lists\n",
    "    vector_size=100,    #size of embedding\n",
    "   window=5,        #context window\n",
    "   min_count=1,     #keep all words (for demo purpose\n",
    "    workers=4,         #choose how much cpu coreses use\n",
    "  sg = 1            # number of skip-grams = 1 since this is small data 1\n",
    ")\n",
    "\n",
    "#train model\n",
    "# model_w2v.train(tokenize_docs, total_examples=len(tokenize_docs), epochs=20)\n",
    "\n",
    "# ex: check similar words\n",
    "# Example: check similar words\n",
    "# model_w2v.wv.most_similar(\"nlp\", topn=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7539d21",
   "metadata": {},
   "source": [
    "#### Implement a GloVe count-based model using the knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f62ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install glove-python-binary\n",
    "\n",
    "from glove import Glove, Corpus\n",
    "import numpy as np\n",
    "\n",
    "sentences = [['this', 'is', 'an', 'example'], ['glove', 'is', 'awesome']]\n",
    "\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=5)\n",
    "\n",
    "glove_model = Glove(no_components=100, learning_rate=0.05)\n",
    "glove_model.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove_model.add_dictionary(corpus.dictionary)\n",
    "\n",
    "print(glove_model.word_vectors[glove_model.dictionary['glove']])\n",
    "print(glove_model.most_similar('glove'))\n",
    "\n",
    "#Download Glove Pretrained Embeddings From: http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "\n",
    "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index.index(word)\n",
    "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "                \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "# matrix for vocab: tokenized_words\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab('../glove.6B.50d/glove.6B.50d.txt', tokenized_words, embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \", embedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab523e1f9dccdb",
   "metadata": {},
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Teams of 2 (individual evaluation in class).\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into markdown comments.\n",
    "\n",
    "\n",
    "## ðŸ§© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** â€“ Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* â€“ NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** â€“ Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
